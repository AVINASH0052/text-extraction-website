{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the stop words list\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Load the positive and negative words dictionaries\n",
    "positive_words = set(open(\"MasterDictionary/positive-words.txt\", \"r\").read().splitlines())\n",
    "negative_words = set(open(\"MasterDictionary/negative-words.txt\", \"r\",encoding = \"ISO-8859-1\").read().splitlines())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean and preprocess the text\n",
    "def preprocess_text(text):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    words = tokenizer.tokenize(text.lower())\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "# Function to count syllables in a word\n",
    "def count_syllables(word):\n",
    "    vowels = 'aeiouy'\n",
    "    count = 0\n",
    "    word = word.lower().strip(\".:;?!\")\n",
    "    if word[0] in vowels:\n",
    "        count += 1\n",
    "    for index in range(1, len(word)):\n",
    "        if word[index] in vowels and word[index - 1] not in vowels:\n",
    "            count += 1\n",
    "    if word.endswith('e'):\n",
    "        count -= 1\n",
    "    if word.endswith('le'):\n",
    "        count += 1\n",
    "    if count == 0:\n",
    "        count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute variables for each article\n",
    "def compute_variables(text):\n",
    "    cleaned_text = preprocess_text(text)\n",
    "    sentences = sent_tokenize(cleaned_text)\n",
    "    words = word_tokenize(cleaned_text)\n",
    "\n",
    "    # Positive Score\n",
    "    positive_score = len([word for word in words if word in positive_words])\n",
    "\n",
    "    # Negative Score\n",
    "    negative_score = len([word for word in words if word in negative_words])\n",
    "\n",
    "    # Polarity Score\n",
    "    polarity_score = (positive_score - negative_score) / (positive_score + negative_score + 0.000001)\n",
    "\n",
    "    # Subjectivity Score\n",
    "    subjectivity_score = (positive_score + negative_score) / (len(words) + 0.000001)\n",
    "\n",
    "    # Average Sentence Length\n",
    "    avg_sentence_length = len(words) / len(sentences)\n",
    "\n",
    "    # Percentage of Complex Words\n",
    "    complex_words = [word for word in words if count_syllables(word) > 2]\n",
    "    percentage_complex_words = len(complex_words) / len(words)\n",
    "\n",
    "    # Fog Index\n",
    "    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)\n",
    "\n",
    "    # Average Number of Words Per Sentence\n",
    "    avg_words_per_sentence = len(words) / len(sentences)\n",
    "\n",
    "    # Complex Word Count\n",
    "    complex_word_count = len(complex_words)\n",
    "\n",
    "    # Word Count\n",
    "    word_count = len(words)\n",
    "\n",
    "    # Syllables Per Word\n",
    "    syllables_per_word = sum(count_syllables(word) for word in words) / len(words)\n",
    "\n",
    "    # Personal Pronouns\n",
    "    personal_pronouns = ['i', 'we', 'my', 'ours', 'us']\n",
    "    personal_pronoun_count = len([word for word in words if word.lower() in personal_pronouns])\n",
    "\n",
    "    # Average Word Length\n",
    "    avg_word_length = sum(len(word) for word in words) / len(words)\n",
    "\n",
    "    return positive_score, negative_score, polarity_score, subjectivity_score, avg_sentence_length, percentage_complex_words, fog_index, avg_words_per_sentence, complex_word_count, word_count, syllables_per_word, personal_pronoun_count, avg_word_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the input data from the Excel file\n",
    "input_data = pd.read_excel('input.xlsx')\n",
    "\n",
    "# Create an empty DataFrame for the output\n",
    "output_data = pd.DataFrame(columns=['URL_ID','URL', 'POSITIVE SCORE', 'NEGATIVE SCORE', 'POLARITY SCORE', 'SUBJECTIVITY SCORE', 'AVG SENTENCE LENGTH', 'PERCENTAGE OF COMPLEX WORDS', 'FOG INDEX', 'AVG NUMBER OF WORDS PER SENTENCE', 'COMPLEX WORD COUNT', 'WORD COUNT', 'SYLLABLE PER WORD', 'PERSONAL PRONOUNS', 'AVG WORD LENGTH'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process each article\n",
    "for index, row in input_data.iterrows():\n",
    "    url_id = row['URL_ID']\n",
    "    url=row['URL']\n",
    "    article_text = open(f'{url_id}.txt', 'r').read()\n",
    "    \n",
    "    # Compute variables for the article\n",
    "    variables = compute_variables(article_text)\n",
    "    \n",
    "    # Add the computed variables to the output DataFrame\n",
    "    output_data.loc[index] = [url_id]+[url] + list(variables)  # type: ignore\n",
    "    \n",
    "\n",
    "# Save the output to the Excel file\n",
    "output_data.to_excel('Output Data Structure.xlsx', index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
